{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Model Examples\n\n",
    "This notebook demonstrates how to use the trained question answering models. We'll load models trained with different architectures and compare their performance on some example questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "from evaluate import load_model\n",
    "from data_utils import simple_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models\n\n",
    "First, let's find and load the latest version of each model type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_model(model_type):\n",
    "    \"\"\"Find the latest model of the specified type.\"\"\"\n",
    "    runs_dir = 'runs'\n",
    "    matching_dirs = [d for d in os.listdir(runs_dir) if d.startswith(model_type)]\n",
    "    if not matching_dirs:\n",
    "        return None\n",
    "    \n",
    "    latest_dir = sorted(matching_dirs)[-1]\n",
    "    model_path = os.path.join(runs_dir, latest_dir, 'best_model.pt')\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        return model_path\n",
    "    return None\n",
    "\n",
    "# Find model paths\n",
    "base_model_path = find_latest_model('base')\n",
    "attention_model_path = find_latest_model('attention')\n",
    "transformer_model_path = find_latest_model('transformer')\n",
    "\n",
    "print(f\"Base model: {base_model_path}\")\n",
    "print(f\"Attention model: {attention_model_path}\")\n",
    "print(f\"Transformer model: {transformer_model_path}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load models\n",
    "models = {}\n",
    "\n",
    "if base_model_path:\n",
    "    print(\"Loading base model...\")\n",
    "    base_model, _ = load_model(base_model_path)\n",
    "    base_model = base_model.to(device)\n",
    "    base_model.eval()\n",
    "    models['base'] = base_model\n",
    "\n",
    "if attention_model_path:\n",
    "    print(\"Loading attention model...\")\n",
    "    attention_model, _ = load_model(attention_model_path)\n",
    "    attention_model = attention_model.to(device)\n",
    "    attention_model.eval()\n",
    "    models['attention'] = attention_model\n",
    "\n",
    "if transformer_model_path:\n",
    "    print(\"Loading transformer model...\")\n",
    "    transformer_model, _ = load_model(transformer_model_path)\n",
    "    transformer_model = transformer_model.to(device)\n",
    "    transformer_model.eval()\n",
    "    models['transformer'] = transformer_model\n",
    "\n",
    "print(f\"\\nLoaded {len(models)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n\n",
    "Let's define some helper functions for generating and visualizing answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(model, model_type, context, question, device):\n",
    "    \"\"\"Get an answer from a model for a given context and question.\"\"\"\n",
    "    # Tokenize the inputs\n",
    "    context_tokens = simple_tokenize(context)\n",
    "    question_tokens = simple_tokenize(question)\n",
    "    \n",
    "    # Truncate if needed\n",
    "    max_context_len = 400\n",
    "    max_question_len = 50\n",
    "    \n",
    "    if len(context_tokens) > max_context_len:\n",
    "        context_tokens = context_tokens[:max_context_len]\n",
    "    if len(question_tokens) > max_question_len:\n",
    "        question_tokens = question_tokens[:max_question_len]\n",
    "    \n",
    "    # For demonstration, we'll use placeholder values for word indices\n",
    "    # In a real scenario, you'd convert tokens to indices using the vocabulary\n",
    "    context_tensor = torch.ones(1, len(context_tokens), dtype=torch.long).to(device)  # All UNK tokens\n",
    "    question_tensor = torch.ones(1, len(question_tokens), dtype=torch.long).to(device)  # All UNK tokens\n",
    "    \n",
    "    context_len = torch.tensor([len(context_tokens)]).to(device)\n",
    "    question_len = torch.tensor([len(question_tokens)]).to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        if model_type == 'base':\n",
    "            start_idx, end_idx = model.predict(context_tensor, context_len, question_tensor, question_len)\n",
    "            attention = None\n",
    "        else:\n",
    "            start_idx, end_idx, attention = model.predict(context_tensor, context_len, question_tensor, question_len)\n",
    "    \n",
    "    # Get the predicted span\n",
    "    start = start_idx.item()\n",
    "    end = end_idx.item()\n",
    "    \n",
    "    # Ensure start <= end\n",
    "    if start > end:\n",
    "        start, end = end, start\n",
    "    \n",
    "    # Extract the answer\n",
    "    answer_tokens = context_tokens[start:end+1]\n",
    "    answer = ' '.join(answer_tokens)\n",
    "    \n",
    "    result = {\n",
    "        'question': question,\n",
    "        'context': context,\n",
    "        'answer': answer,\n",
    "        'start': start,\n",
    "        'end': end,\n",
    "        'context_tokens': context_tokens\n",
    "    }\n",
    "    \n",
    "    if attention is not None:\n",
    "        result['attention'] = attention[0].cpu().numpy()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def highlight_answer(context, start, end):\n",
    "    \"\"\"Highlight the answer in the context for display.\"\"\"\n",
    "    tokens = simple_tokenize(context)\n",
    "    highlighted = []\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        if start <= i <= end:\n",
    "            highlighted.append(f\"<b style='color:red'>{token}</b>\")\n",
    "        else:\n",
    "            highlighted.append(token)\n",
    "    \n",
    "    return ' '.join(highlighted)\n",
    "\n",
    "def visualize_attention(context_tokens, attention_weights, title=\"Attention Weights\"):\n",
    "    \"\"\"Visualize attention weights.\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.imshow(attention_weights, cmap='viridis')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Context Position')\n",
    "    \n",
    "    # Show tokens on x-axis\n",
    "    if len(context_tokens) <= 50:  # Only show tokens if not too many\n",
    "        plt.xticks(range(len(context_tokens)), context_tokens, rotation=90)\n",
    "    \n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Example 1: Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example context and question about deep learning\n",
    "context1 = \"\"\"\n",
    "Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. \n",
    "Learning can be supervised, semi-supervised or unsupervised. Deep learning architectures such as deep neural networks, \n",
    "deep belief networks, recurrent neural networks and convolutional neural networks have been applied to fields including \n",
    "computer vision, speech recognition, natural language processing, audio recognition, social network filtering, \n",
    "machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, \n",
    "where they have produced results comparable to and in some cases surpassing human expert performance.\n",
    "Transformer architectures, a type of deep learning model, have become particularly important for NLP tasks \n",
    "since their introduction in 2017. Transformers use self-attention mechanisms to process input sequences in parallel, \n",
    "which has proven especially effective for tasks like machine translation, text summarization, and question answering. \n",
    "Models like BERT, GPT, and T5 are all based on the transformer architecture and have set new performance benchmarks \n",
    "across numerous language understanding tasks.\n",
    "\"\"\"\n",
    "\n",
    "questions1 = [\n",
    "    \"What is deep learning part of?\",\n",
    "    \"What types of learning can be used in deep learning?\",\n",
    "    \"In which fields has deep learning been applied?\"\n",
    "]\n",
    "\n",
    "# Test all models with the questions\n",
    "for question in questions1:\n",
    "    print(f\"\\n{'='*80}\\nQuestion: {question}\\n{'='*80}\\n\")\n",
    "    \n",
    "    for model_type, model in models.items():\n",
    "        print(f\"Model: {model_type.upper()}\")\n",
    "        result = get_answer(model, model_type, context1, question, device)\n",
    "        \n",
    "        print(f\"Answer: {result['answer']}\\n\")\n",
    "        display(HTML(f\"<p>Context with highlighted answer:</p><p>{highlight_answer(context1, result['start'], result['end'])}</p>\"))\n",
    "        \n",
    "        # Visualize attention if available\n",
    "        if 'attention' in result and model_type != 'base':\n",
    "            visualize_attention(result['context_tokens'], result['attention'], f\"{model_type.capitalize()} Attention\")\n",
    "        \n",
    "        print('\\n' + '-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Example 2: Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example context about NLP\n",
    "context2 = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence \n",
    "concerned with the interactions between computers and human language, in particular how to program computers \n",
    "to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" \n",
    "the contents of documents, including the contextual nuances of the language within them. The technology can then \n",
    "accurately extract information and insights contained in the documents as well as categorize and organize the \n",
    "documents themselves. Challenges in natural language processing frequently involve speech recognition, natural \n",
    "language understanding, and natural language generation. Modern NLP approaches are based on machine learning, \n",
    "especially statistical methods and neural networks. As of 2020, deep learning approaches such as transformers \n",
    "have achieved state-of-the-art results on many NLP tasks.\n",
    "Question answering (QA) is an important NLP task that involves automatically answering questions posed in natural language. \n",
    "Machine reading comprehension, a subset of QA, focuses on answering questions based on a given context passage. \n",
    "The Stanford Question Answering Dataset (SQuAD) has become a benchmark dataset for this task, consisting of questions \n",
    "posed by crowdworkers on a set of Wikipedia articles. In SQuAD, the answer to every question is a segment of text \n",
    "from the corresponding reading passage. Models are evaluated based on exact match and F1 scores, comparing their \n",
    "predicted answers against human-provided reference answers.\n",
    "\"\"\"\n",
    "\n",
    "questions2 = [\n",
    "    \"What is NLP?\",\n",
    "    \"What are the challenges in natural language processing?\",\n",
    "    \"What approaches do modern NLP systems use?\"\n",
    "]\n",
    "\n",
    "# Test with attention model only for brevity\n",
    "if models:\n",
    "    model_type = 'attention' if 'attention' in models else list(models.keys())[0]\n",
    "    model = models[model_type]\n",
    "    for question in questions2:\n",
    "        print(f\"\\n{'='*80}\\nQuestion: {question}\\n{'='*80}\\n\")\n",
    "        \n",
    "        result = get_answer(model, model_type, context2, question, device)\n",
    "        print(f\"Answer: {result['answer']}\\n\")\n",
    "        display(HTML(f\"<p>Context with highlighted answer:</p><p>{highlight_answer(context2, result['start'], result['end'])}</p>\"))\n",
    "        \n",
    "        # Visualize attention if available\n",
    "        if 'attention' in result and model_type != 'base':\n",
    "            visualize_attention(result['context_tokens'], result['attention'], f\"{model_type.capitalize()} Attention\")\n",
    "        \n",
    "        print('\\n' + '-'*80)\n",
    "else:\n",
    "    print(\"No models loaded, cannot run example 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Your Own Question\n\n",
    "Now you can try your own context and question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your own context and question\n",
    "my_context = \"\"\"\n",
    "PyTorch is an open source machine learning framework based on the Torch library, \n",
    "used for applications such as computer vision and natural language processing, \n",
    "originally developed by Meta AI and now part of the Linux Foundation umbrella. \n",
    "It is free and open-source software released under the Modified BSD license. \n",
    "Although the Python interface is more polished and the primary focus of development, \n",
    "PyTorch also has a C++ interface. PyTorch provides two high-level features: \n",
    "Tensor computing (like NumPy) with strong acceleration via graphics processing units (GPU) \n",
    "and Deep neural networks built on a tape-based automatic differentiation system.\n",
    "PyTorch is distinctive in its implementation of dynamic computational graphs, which allow for \n",
    "more flexible model building compared to static graph frameworks. This 'define-by-run' approach \n",
    "enables developers to modify neural networks on the fly, making debugging and experimentation easier. \n",
    "The framework includes modules for building complex neural network architectures, optimizers for \n",
    "training, data loading utilities, and seamless GPU integration. Its ecosystem has expanded \n",
    "to include libraries like torchvision for computer vision, torchaudio for audio processing, \n",
    "torchtext for NLP, and PyTorch Lightning for organizing research code. With its intuitive design \n",
    "and Python-native flow, PyTorch has become especially popular in research communities.\n",
    "\"\"\"\n",
    "\n",
    "my_question = \"Who developed PyTorch?\"\n",
    "\n",
    "# Choose which model to use (using attention model if available)\n",
    "if models:\n",
    "    model_type = 'attention' if 'attention' in models else list(models.keys())[0]\n",
    "    model = models[model_type]\n",
    "    \n",
    "    # Get answer\n",
    "    result = get_answer(model, model_type, my_context, my_question, device)\n",
    "    \n",
    "    print(f\"Question: {result['question']}\\n\")\n",
    "    print(f\"Answer: {result['answer']}\\n\")\n",
    "    display(HTML(f\"<p>Context with highlighted answer:</p><p>{highlight_answer(my_context, result['start'], result['end'])}</p>\"))\n",
    "    \n",
    "    # Visualize attention if available\n",
    "    if 'attention' in result and model_type != 'base':\n",
    "        visualize_attention(result['context_tokens'], result['attention'], f\"{model_type.capitalize()} Attention\")\n",
    "else:\n",
    "    print(\"No models loaded, cannot answer custom question.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}