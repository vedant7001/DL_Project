{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae441c15",
   "metadata": {},
   "source": [
    "# Model Comparison Visualization\n",
    "\n",
    "This notebook provides a comprehensive visualization and analysis of three different question answering model architectures:\n",
    "1. Base Model (LSTM/GRU without Attention)\n",
    "2. Attention Model (with Bahdanau Attention)\n",
    "3. Transformer Model (with Self-Attention)\n",
    "\n",
    "We'll analyze various aspects including:\n",
    "- Performance metrics (F1 score, inference time)\n",
    "- Model complexity\n",
    "- Training and inference speeds\n",
    "- Model characteristics\n",
    "- Resource utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e88f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Set style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"muted\")\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569fd395",
   "metadata": {},
   "source": [
    "## Model Data\n",
    "\n",
    "First, let's define our model comparison data. This includes various metrics and scores for each model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e48ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison data\n",
    "models = {\n",
    "    \"Base (LSTM/GRU)\": {\n",
    "        \"accuracy\": 0.0375,  # F1 score\n",
    "        \"exact_match\": 0.0,\n",
    "        \"training_time_per_epoch\": 1.77,  # seconds\n",
    "        \"inference_time\": 207.22,  # milliseconds\n",
    "        \"parameters\": 263810,\n",
    "        \"memory_usage\": 1.0,  # relative value\n",
    "        \"interpretability\": 2,  # score out of 10\n",
    "        \"parallelization\": 2,  # score out of 10\n",
    "        \"scalability\": 3,  # score out of 10\n",
    "        \"ease_of_implementation\": 8,  # score out of 10\n",
    "        \"convergence_speed\": 5,  # score out of 10\n",
    "        \"context_length_handling\": 4  # score out of 10\n",
    "    },\n",
    "    \"Attention (Bahdanau)\": {\n",
    "        \"accuracy\": 0.04,\n",
    "        \"exact_match\": 0.0,\n",
    "        \"training_time_per_epoch\": 1.68,\n",
    "        \"inference_time\": 180.49,\n",
    "        \"parameters\": 272066,\n",
    "        \"memory_usage\": 1.2,\n",
    "        \"interpretability\": 8,\n",
    "        \"parallelization\": 2,\n",
    "        \"scalability\": 5,\n",
    "        \"ease_of_implementation\": 6,\n",
    "        \"convergence_speed\": 6,\n",
    "        \"context_length_handling\": 6\n",
    "    },\n",
    "    \"Transformer\": {\n",
    "        \"accuracy\": 0.069,\n",
    "        \"exact_match\": 0.0,\n",
    "        \"training_time_per_epoch\": 9.79,\n",
    "        \"inference_time\": 187.84,\n",
    "        \"parameters\": 1821954,\n",
    "        \"memory_usage\": 6.7,\n",
    "        \"interpretability\": 6,\n",
    "        \"parallelization\": 9,\n",
    "        \"scalability\": 8,\n",
    "        \"ease_of_implementation\": 4,\n",
    "        \"convergence_speed\": 7,\n",
    "        \"context_length_handling\": 8\n",
    "    }\n",
    "}\n",
    "\n",
    "# Helper function for formatting large numbers\n",
    "def format_with_commas(x, pos):\n",
    "    return f'{int(x):,}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5a6c8",
   "metadata": {},
   "source": [
    "## 1. Basic Performance Metrics\n",
    "\n",
    "Let's start by visualizing the key performance metrics: F1 score, training time, and inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d82ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_basic_metrics():\n",
    "    metrics = ['accuracy', 'training_time_per_epoch', 'inference_time']\n",
    "    titles = ['F1 Score', 'Training Time per Epoch (s)', 'Inference Time (ms)']\n",
    "    colors = ['#3498db', '#f39c12', '#2ecc71']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "        values = [models[model][metric] for model in models]\n",
    "        axes[i].bar(models.keys(), values, color=colors)\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].set_ylabel(metric.replace('_', ' ').title())\n",
    "        if metric == 'accuracy':\n",
    "            for j, v in enumerate(values):\n",
    "                axes[i].text(j, v + 0.001, f\"{v:.4f}\", ha='center')\n",
    "        else:\n",
    "            for j, v in enumerate(values):\n",
    "                axes[i].text(j, v + 0.1, f\"{v:.2f}\", ha='center')\n",
    "        \n",
    "        plt.setp(axes[i].xaxis.get_majorticklabels(), rotation=30, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "plot_basic_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c0a974",
   "metadata": {},
   "source": [
    "### Analysis of Basic Metrics\n",
    "\n",
    "- **F1 Score**: The Transformer model achieves the highest F1 score (0.069), followed by the Attention model (0.04) and Base model (0.0375).\n",
    "- **Training Time**: The Transformer model takes significantly longer to train (9.79s/epoch) compared to the other models (~1.7s/epoch).\n",
    "- **Inference Time**: Interestingly, the Attention model has the fastest inference time (180.49ms), while the Base model is the slowest (207.22ms)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792dd28d",
   "metadata": {},
   "source": [
    "## 2. Model Complexity\n",
    "\n",
    "Let's examine the number of parameters in each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e7c97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_complexity():\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    model_names = list(models.keys())\n",
    "    params = [models[model]['parameters'] for model in model_names]\n",
    "    \n",
    "    bars = ax.bar(model_names, params, color=['#3498db', '#f39c12', '#2ecc71'])\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 50000,\n",
    "                f'{int(height):,}', ha='center', va='bottom')\n",
    "    \n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(format_with_commas))\n",
    "    ax.set_ylabel('Number of Parameters')\n",
    "    ax.set_title('Model Complexity Comparison')\n",
    "    \n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=30, ha='right')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "plot_model_complexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03259801",
   "metadata": {},
   "source": [
    "### Analysis of Model Complexity\n",
    "\n",
    "- The Transformer model has significantly more parameters (1.8M) compared to the other models.\n",
    "- The Attention model adds only a small number of parameters (~8K) to the Base model.\n",
    "- This shows that attention mechanisms can provide significant improvements with minimal parameter increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc1049d",
   "metadata": {},
   "source": [
    "## 3. Model Characteristics Radar Chart\n",
    "\n",
    "Let's visualize various qualitative characteristics of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0698c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_radar_chart():\n",
    "    features = ['interpretability', 'parallelization', 'scalability', \n",
    "                'ease_of_implementation', 'convergence_speed', 'context_length_handling']\n",
    "    \n",
    "    model_names = list(models.keys())\n",
    "    \n",
    "    # Number of features\n",
    "    N = len(features)\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    feature_names = [f.replace('_', ' ').title() for f in features]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    for i, model in enumerate(model_names):\n",
    "        values = [models[model][feature] for feature in features]\n",
    "        values += values[:1]\n",
    "        \n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=model)\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(feature_names)\n",
    "    ax.set_ylim(0, 10)\n",
    "    \n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title('Model Characteristics Comparison', size=15, y=1.1)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "plot_radar_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12925eb5",
   "metadata": {},
   "source": [
    "### Analysis of Model Characteristics\n",
    "\n",
    "- **Interpretability**: The Attention model excels due to its clear attention maps.\n",
    "- **Parallelization**: The Transformer model significantly outperforms the others due to its non-sequential nature.\n",
    "- **Scalability**: Transformer shows the best scaling potential for larger datasets and contexts.\n",
    "- **Ease of Implementation**: The Base model is simplest to implement, while the Transformer is most complex.\n",
    "- **Context Length Handling**: Transformer handles long contexts best, followed by the Attention model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddf33e0",
   "metadata": {},
   "source": [
    "## 4. Performance vs. Resource Trade-off\n",
    "\n",
    "Let's create a bubble chart to visualize the relationship between accuracy, training time, and model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff28c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bubble_chart():\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    model_names = list(models.keys())\n",
    "    x = [models[model]['training_time_per_epoch'] for model in model_names]\n",
    "    y = [models[model]['accuracy'] for model in model_names]\n",
    "    size = [np.sqrt(models[model]['parameters'])/30 for model in model_names]\n",
    "    colors = ['#3498db', '#f39c12', '#2ecc71']\n",
    "    \n",
    "    scatter = ax.scatter(x, y, s=size, c=colors, alpha=0.6, edgecolors='black')\n",
    "    \n",
    "    for i, model in enumerate(model_names):\n",
    "        ax.annotate(model, (x[i], y[i]),\n",
    "                   xytext=(10, 5), textcoords='offset points')\n",
    "    \n",
    "    ax.set_xlabel('Training Time per Epoch (s)')\n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.set_title('Accuracy vs Training Time vs Model Size')\n",
    "    \n",
    "    ax.text(0.95, 0.05, 'Bubble size represents\\nnumber of parameters',\n",
    "            transform=ax.transAxes, fontsize=9,\n",
    "            verticalalignment='bottom', horizontalalignment='right',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "plot_bubble_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaa082c",
   "metadata": {},
   "source": [
    "### Analysis of Trade-offs\n",
    "\n",
    "- The Transformer model achieves the highest accuracy but requires significantly more parameters and training time.\n",
    "- The Attention model provides a good balance, achieving better accuracy than the base model with minimal parameter increase.\n",
    "- The Base model is most efficient in terms of parameters but has the lowest performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8f29c6",
   "metadata": {},
   "source": [
    "## 5. Speed Metrics Comparison\n",
    "\n",
    "Let's compare training and inference speeds across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a33c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_speed_metrics():\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    model_names = list(models.keys())\n",
    "    train_times = [models[model]['training_time_per_epoch'] for model in model_names]\n",
    "    inference_times = [models[model]['inference_time']/1000 for model in model_names]\n",
    "    \n",
    "    bar_width = 0.35\n",
    "    x = np.arange(len(model_names))\n",
    "    \n",
    "    ax.bar(x, train_times, bar_width, label='Training Time per Epoch (s)', color='#3498db')\n",
    "    ax.bar(x, inference_times, bar_width, bottom=train_times, \n",
    "           label='Inference Time (s)', color='#f39c12')\n",
    "    \n",
    "    ax.set_ylabel('Time (seconds)')\n",
    "    ax.set_title('Training and Inference Speed Comparison')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names)\n",
    "    ax.legend()\n",
    "    \n",
    "    for i in range(len(model_names)):\n",
    "        ax.text(i, train_times[i]/2, f'{train_times[i]:.2f}s', ha='center', va='center', color='white')\n",
    "        ax.text(i, train_times[i] + inference_times[i]/2, f'{inference_times[i]:.3f}s', \n",
    "                ha='center', va='center', color='white')\n",
    "    \n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=30, ha='right')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "plot_speed_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead3e2b2",
   "metadata": {},
   "source": [
    "### Analysis of Speed Metrics\n",
    "\n",
    "- Training time varies significantly between models, with the Transformer being much slower.\n",
    "- Inference times are more comparable across models.\n",
    "- The Attention model shows surprisingly good inference speed despite its additional complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5501b9a0",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "1. **Performance vs. Complexity Trade-off**:\n",
    "   - Transformer: Best performance but highest complexity\n",
    "   - Attention: Good balance of performance and complexity\n",
    "   - Base: Simplest but lowest performance\n",
    "\n",
    "2. **Resource Considerations**:\n",
    "   - For resource-constrained environments, the Attention model offers the best balance\n",
    "   - For maximum performance, the Transformer model is preferred\n",
    "   - The Base model is suitable for simple applications or as a baseline\n",
    "\n",
    "3. **Practical Recommendations**:\n",
    "   - Production systems: Transformer model with optimization techniques\n",
    "   - Resource-constrained: Attention model\n",
    "   - Rapid prototyping: Base model\n",
    "   - Interpretability needs: Attention model"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
